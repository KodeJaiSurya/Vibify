name: Data Updation

on:
  schedule:
    # Runs at 00:00 on the 1st of every month
    - cron: '0 0 1 * *'
  workflow_dispatch:  # Allows manual trigger

env:
  GCS_BUCKET: ${{ secrets.GCS_BUCKET }}

jobs:
  update-dataset:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dataset:
          - {
              name: 'facial-expression',
              kaggle_id: 'nicolejyt/facialexpressionrecognition',
              gcs_path: 'data/raw/facial_expression',
              specific_file: ''
            }
          - {
              name: 'spotify-genres',
              kaggle_id: 'mrmorj/dataset-of-songs-in-spotify',
              gcs_path: 'data/raw/spotify',
              specific_file: 'genres_v2.csv'
            }
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "==3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install kaggle google-cloud-storage tensorflow-data-validation pandas
        pip install -r requirements.txt

    - name: Configure Kaggle credentials
      run: |
        mkdir -p ~/.kaggle
        echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\",\"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
        chmod 600 ~/.kaggle/kaggle.json

    - name: Configure GCP credentials
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_KEY }}

    
    - name: Initialize GCS folders
      run: python pipelines/dags/src/gcs_init.py ${{ env.GCS_BUCKET }}
  
    - name: Sync src directory to GCS
      run: python pipelines/dags/src/gcs_sync.py ${{ env.GCS_BUCKET }} pipelines/dags/src

    - name: Download Dataset
      run: |
        kaggle datasets download -d ${{ matrix.dataset.kaggle_id }} --unzip -p ./data
        echo "Dataset contents:"
        ls -la ./data

    - name: Run Anomaly Detection
      run: python pipelines/dags/src/anomaly_detection.py ./data '${{ matrix.dataset.specific_file }}'
  
    - name: Upload to GCS with replacement
      run: python pipelines/dags/src/gcs_upload.py '${{ env.GCS_BUCKET }}' './data' '${{ matrix.dataset.gcs_path }}' '${{ matrix.dataset.specific_file }}'

    - name: Bias Mitigation
      run: python pipelines/dags/src/bias_mitigation.py '${{ env.GCS_BUCKET }}' './data' '${{ matrix.dataset.specific_file }}'

    - name: Cleanup local files
      if: always()
      run: |
        rm -rf ./data
        echo "Cleaned up local files"

    - name: Trigger Data Pipeline
      if: success()
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'data_pipeline.yml',
            ref: context.ref
          })